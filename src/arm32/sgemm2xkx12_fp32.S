    .equ      VERSION_MAJOR,    1
    .equ      VERSION_MINOR,    0
    .equ      VERSION_REVISION, 0

    .equ      PHASE,            1
    .equ      COPYRIGHT_YEAR,   2018

COPYRIGHT_HOLDER:
    .asciz    "tianylijun@163.com"
    .equ      NE_OK,        0
    .equ      NE_ERR,      -1

#define STACK_SIZE       512

/* RSV [r4~r9,fp] */
/* void sgemm2xKx12_fp32(float *pA, float *pB, float *pC, uint32_t K, uint32_t N, uint32_t reluType, float *pPrelu, uint32_t bSharedPrelu, float *pBasis) */
/**************in param**************/
#define A                r0
#define B                r1
#define C                r2
#define K                r3

/********** Backup R Regs ***********/
#define N                r4
#define reluType         r5
#define pPrelu           r6
#define bSharedPrelu     r7
#define pBasis           r8
#define KDiv4            r9
#define KHas2            r9
#define KHas1            r9

/************ Stack Param ***********/
#define ST_N              [fp, #0]
#define ST_bRelu          [fp, #4]
#define ST_pPrelu         [fp, #8]
#define ST_bSharedPrelu   [fp, #12]
#define ST_pBasis         [fp, #16]

/************ Vector Regs ***********/
/* RSV Q0~Q7 */
#define VSRC_2S_A        d0
#define VSRC_2S_A_0      d0[0]
#define VSRC_2S_A_1      d0[1]

#define VSRC_4S_A0       q0
#define VSRC_4S_A0_0     d0[0]
#define VSRC_4S_A0_1     d0[1]
#define VSRC_4S_A0_2     d1[0]
#define VSRC_4S_A0_3     d1[1]

#define VSRC_4S_A1       q1
#define VSRC_4S_A1_0     d2[0]
#define VSRC_4S_A1_1     d2[1]
#define VSRC_4S_A1_2     d3[0]
#define VSRC_4S_A1_3     d3[1]

#define VSRC_4S_B0       q2
#define VSRC_4S_B1       q3
#define VSRC_4S_B2       q4

#define VBASIS_2S        d0
#define VBASIS_2S_0      d0[0]
#define VBASIS_2S_1      d0[1]

#define VBASIS_2S_DUP_0  q0
#define VBASIS_2S_DUP_1  q1

#define VSIX_4S          q0
#define VMASK            q0
#define VZERO_4S         q1
#define VSCALE_4S        q2
#define VSCALE_2S        d4
#define VSCALE_2S_LANE_0 d4[]
#define VSCALE_2S_0      d4[0]
#define VSCALE_2S_1      d4[1]
#define VMUL_4S          q3

#define VSRC_4S_C0_0     q10
#define VSRC_4S_C0_1     q11
#define VSRC_4S_C0_2     q12

#define VSRC_4S_C1_0     q13
#define VSRC_4S_C1_1     q14
#define VSRC_4S_C1_2     q15

/************ Stack fp Area *********/
#define  STACK_START  [fp, #-540] // -512-28

/*
----------------------------------------------------------------------------------------------
            |                                                           |          ^
            |                                                           |          ^
            |                                                           |          ^
NEW_SP(TOP)-|--------------L ADDR----------------|-->[fp - 512 - 28] ---|--------PUSH BASE---
            |                                    |                      |
            |              (512-128)             |                      |
            |                                    |                      |
FP - 156----|------------RSV(128)---STACK_END----|    STACK_SIZE(512)   |
            |                                    |                      |
            |             s0~s31                 |                      |
            |                                    |                      |
PUSH_SP-----|------------------------------------|-----------------------
            |                                    |
            |        (R4~R9, FP) 28 Bytes        |
            |                                    |
0LD_SP FP --|------------------------------------|
            |          PARM_0(FP+0)              |
            |          PARM_1(FP+4)              |
            |          PARM_2(FP+8)              |
            |          PARM_3(FP+12)             |
            |               ...                  |
            |                                    |
---------------------------H ADDR------------------------------------------------------------------

ABI: hard    r0 r1 r2 r3  [fp,#0]  [fp,#4]  [s0]      [s0]      [fp,#8]   [fp,#12]  [fp,#16] [fp,#20]
ABI: softfp  r0 r1 r2 r3  [fp,#0]  [fp,#4]  [fp,#8]   [fp,#12]  [fp,#16]  [fp,#20]
*/

/* void sgemm2xKx12_fp32(float *pA, float *pB, float *pC, uint32_t K, uint32_t N, uint32_t reluType, float *pPrelu, uint32_t bSharedPrelu, float *pBasis) */
    .text
    .align 5
#ifdef __APPLE__
    .global _sgemm2xKx12_fp32
_sgemm2xKx12_fp32:
#else
    .global sgemm2xKx12_fp32
sgemm2xKx12_fp32:
#endif
    push {r4-r9, fp}
    add fp, sp, #28
    sub sp, sp, #STACK_SIZE
    sub r4, fp, #156                   /* [fp, -156] */
    vstm r4, {s0-s19}

    ldr N, ST_N                        /* load param from stack */
    ldr reluType, ST_bRelu                /* load param from stack */
    lsl N, N, #2
    ldr pPrelu, ST_pPrelu              /* load param from stack */
    lsr KDiv4, K, #2
    ldr bSharedPrelu, ST_bSharedPrelu  /* load param from stack */
    ldr pBasis, ST_pBasis              /* load param from stack */

    pld [B, #32]
    veor VSRC_4S_C0_0, VSRC_4S_C0_0, VSRC_4S_C0_0
    pld [A, #32]
    veor VSRC_4S_C0_1, VSRC_4S_C0_1, VSRC_4S_C0_1
    vmov VSRC_4S_C0_2, VSRC_4S_C0_0
    veor VSRC_4S_C1_0, VSRC_4S_C1_0, VSRC_4S_C1_0
    vmov VSRC_4S_C1_1, VSRC_4S_C0_0
    veor VSRC_4S_C1_2, VSRC_4S_C1_2, VSRC_4S_C1_2

    cmp KDiv4, #0
    beq __KHAS2

__LOOP:
    /* 0 */
    vld1.32 {VSRC_4S_B0, VSRC_4S_B1}, [B:128]!
    subs KDiv4, KDiv4, #1
    vld1.32 {VSRC_4S_A0, VSRC_4S_A1}, [A]!

    vmla.f32 VSRC_4S_C0_0, VSRC_4S_B0, VSRC_4S_A0_0
    vld1.32 {VSRC_4S_B2}, [B:128]!
    vmla.f32 VSRC_4S_C0_1, VSRC_4S_B1, VSRC_4S_A0_0
    pld [B, #64]
    vmla.f32 VSRC_4S_C1_0, VSRC_4S_B0, VSRC_4S_A0_1
    vmla.f32 VSRC_4S_C1_1, VSRC_4S_B1, VSRC_4S_A0_1

    vmla.f32 VSRC_4S_C0_2, VSRC_4S_B2, VSRC_4S_A0_0
    vld1.32 {VSRC_4S_B0, VSRC_4S_B1}, [B:128]!
    vmla.f32 VSRC_4S_C1_2, VSRC_4S_B2, VSRC_4S_A0_1

    /* 1 */
    vmla.f32 VSRC_4S_C0_0, VSRC_4S_B0, VSRC_4S_A0_2
    vld1.32 {VSRC_4S_B2}, [B:128]!
    vmla.f32 VSRC_4S_C0_1, VSRC_4S_B1, VSRC_4S_A0_2
    pld [B, #64]
    vmla.f32 VSRC_4S_C1_0, VSRC_4S_B0, VSRC_4S_A0_3
    vmla.f32 VSRC_4S_C1_1, VSRC_4S_B1, VSRC_4S_A0_3

    vmla.f32 VSRC_4S_C0_2, VSRC_4S_B2, VSRC_4S_A0_2
    vld1.32 {VSRC_4S_B0, VSRC_4S_B1}, [B:128]!
    vmla.f32 VSRC_4S_C1_2, VSRC_4S_B2, VSRC_4S_A0_3

    /* 2 */
    pld [A, #32]
    vmla.f32 VSRC_4S_C0_0, VSRC_4S_B0, VSRC_4S_A1_0
    vld1.32 {VSRC_4S_B2}, [B:128]!
    vmla.f32 VSRC_4S_C0_1, VSRC_4S_B1, VSRC_4S_A1_0
    pld [B, #64]
    vmla.f32 VSRC_4S_C1_0, VSRC_4S_B0, VSRC_4S_A1_1
    vmla.f32 VSRC_4S_C1_1, VSRC_4S_B1, VSRC_4S_A1_1

    vmla.f32 VSRC_4S_C0_2, VSRC_4S_B2, VSRC_4S_A1_0
    vld1.32 {VSRC_4S_B0, VSRC_4S_B1}, [B:128]!
    vmla.f32 VSRC_4S_C1_2, VSRC_4S_B2, VSRC_4S_A1_1

    /* 3 */
    vmla.f32 VSRC_4S_C0_0, VSRC_4S_B0, VSRC_4S_A1_2
    vld1.32 {VSRC_4S_B2}, [B:128]!
    vmla.f32 VSRC_4S_C0_1, VSRC_4S_B1, VSRC_4S_A1_2
    pld [B, #64]
    vmla.f32 VSRC_4S_C1_0, VSRC_4S_B0, VSRC_4S_A1_3
    vmla.f32 VSRC_4S_C1_1, VSRC_4S_B1, VSRC_4S_A1_3

    vmla.f32 VSRC_4S_C0_2, VSRC_4S_B2, VSRC_4S_A1_2
    cmp KDiv4, #0
    vmla.f32 VSRC_4S_C1_2, VSRC_4S_B2, VSRC_4S_A1_3

    bne __LOOP

__KHAS2:
    and KHas2, K, #2
    cmp KHas2, #0
    beq __KHAS1

    /* 0 */
    vld1.32 {VSRC_4S_B0, VSRC_4S_B1}, [B:128]!
    vld1.32 {VSRC_4S_A0}, [A]!

    vmla.f32 VSRC_4S_C0_0, VSRC_4S_B0, VSRC_4S_A0_0
    vld1.32 {VSRC_4S_B2}, [B:128]!
    vmla.f32 VSRC_4S_C0_1, VSRC_4S_B1, VSRC_4S_A0_0
    pld [B, #64]
    vmla.f32 VSRC_4S_C1_0, VSRC_4S_B0, VSRC_4S_A0_1
    vmla.f32 VSRC_4S_C1_1, VSRC_4S_B1, VSRC_4S_A0_1

    vmla.f32 VSRC_4S_C0_2, VSRC_4S_B2, VSRC_4S_A0_0
    vld1.32 {VSRC_4S_B0, VSRC_4S_B1}, [B:128]!
    vmla.f32 VSRC_4S_C1_2, VSRC_4S_B2, VSRC_4S_A0_1

    /* 1 */
    vmla.f32 VSRC_4S_C0_0, VSRC_4S_B0, VSRC_4S_A0_2
    vld1.32 {VSRC_4S_B2}, [B:128]!
    vmla.f32 VSRC_4S_C0_1, VSRC_4S_B1, VSRC_4S_A0_2
    pld [B, #64]
    vmla.f32 VSRC_4S_C1_0, VSRC_4S_B0, VSRC_4S_A0_3
    vmla.f32 VSRC_4S_C1_1, VSRC_4S_B1, VSRC_4S_A0_3
    pld [A, #16]

    vmla.f32 VSRC_4S_C0_2, VSRC_4S_B2, VSRC_4S_A0_2
    vmla.f32 VSRC_4S_C1_2, VSRC_4S_B2, VSRC_4S_A0_3

__KHAS1:
    and KHas1, K, #1
    cmp KHas1, #0
    beq __BASIS

    /* 0 */
    vld1.32 {VSRC_4S_B0, VSRC_4S_B1}, [B:128]!
    vld1.32 {VSRC_2S_A}, [A]

    vmla.f32 VSRC_4S_C0_0, VSRC_4S_B0, VSRC_2S_A_0
    vld1.32 {VSRC_4S_B2}, [B:128]
    vmla.f32 VSRC_4S_C0_1, VSRC_4S_B1, VSRC_2S_A_0
    vmla.f32 VSRC_4S_C1_0, VSRC_4S_B0, VSRC_2S_A_1
    vmla.f32 VSRC_4S_C1_1, VSRC_4S_B1, VSRC_2S_A_1

    vmla.f32 VSRC_4S_C0_2, VSRC_4S_B2, VSRC_2S_A_0
    vmla.f32 VSRC_4S_C1_2, VSRC_4S_B2, VSRC_2S_A_1

__BASIS:
    cmp pBasis, #0
    beq __RELU

    vld1.32 {VBASIS_2S}, [pBasis]
    vdup.32 VBASIS_2S_DUP_1, VBASIS_2S_1

    vadd.f32 VSRC_4S_C1_0, VSRC_4S_C1_0, VBASIS_2S_DUP_1
    vdup.32 VBASIS_2S_DUP_0, VBASIS_2S_0
    vadd.f32 VSRC_4S_C1_1, VSRC_4S_C1_1, VBASIS_2S_DUP_1
    vadd.f32 VSRC_4S_C1_2, VSRC_4S_C1_2, VBASIS_2S_DUP_1

    vadd.f32 VSRC_4S_C0_0, VSRC_4S_C0_0, VBASIS_2S_DUP_0
    vadd.f32 VSRC_4S_C0_1, VSRC_4S_C0_1, VBASIS_2S_DUP_0
    vadd.f32 VSRC_4S_C0_2, VSRC_4S_C0_2, VBASIS_2S_DUP_0

__RELU:
    cmp reluType, #0
    beq __PRELU

    veor VZERO_4S, VZERO_4S, VZERO_4S

    cmp reluType, #2
    beq __RELU6

.macro RELU_MACRO, src_0:req
    vmax.f32 \src_0, \src_0, VZERO_4S
.endm
    pld [C, #48]
    RELU_MACRO VSRC_4S_C0_0
    RELU_MACRO VSRC_4S_C0_1
    RELU_MACRO VSRC_4S_C0_2

    RELU_MACRO VSRC_4S_C1_0
    RELU_MACRO VSRC_4S_C1_1
    RELU_MACRO VSRC_4S_C1_2

    b __STORE

__RELU6:
    vmov.f32 VSIX_4S, #6.0

.macro RELU6_MACRO, src_0:req
    vmax.f32 \src_0, \src_0, VZERO_4S
    vmin.f32 \src_0, \src_0, VSIX_4S
.endm
    pld [C, #48]
    RELU6_MACRO VSRC_4S_C0_0
    RELU6_MACRO VSRC_4S_C0_1
    RELU6_MACRO VSRC_4S_C0_2

    RELU6_MACRO VSRC_4S_C1_0
    RELU6_MACRO VSRC_4S_C1_1
    RELU6_MACRO VSRC_4S_C1_2

    b __STORE

__PRELU:
    cmp pPrelu, #0
    beq __STORE

    veor VZERO_4S, VZERO_4S, VZERO_4S

    cmp bSharedPrelu, #0
    beq __SEPARATE

    vld1.32 {VSCALE_2S_LANE_0}, [pPrelu]
    b __PRELU_BEG

__SEPARATE:
    vld1.32 {VSCALE_2S}, [pPrelu]

__PRELU_BEG:
.macro PRELU_MACRO, src_0:req src_1:req
    vcle.f32 VMASK, \src_0, VZERO_4S
    vmul.f32 VMUL_4S, \src_0, \src_1
    vbsl VMASK, VMUL_4S, \src_0
    vmov \src_0, VMASK
.endm

    pld [C, #48]
    PRELU_MACRO VSRC_4S_C0_0 VSCALE_2S_0
    PRELU_MACRO VSRC_4S_C0_1 VSCALE_2S_0
    PRELU_MACRO VSRC_4S_C0_2 VSCALE_2S_0

    PRELU_MACRO VSRC_4S_C1_0 VSCALE_2S_1
    PRELU_MACRO VSRC_4S_C1_1 VSCALE_2S_1
    PRELU_MACRO VSRC_4S_C1_2 VSCALE_2S_1

__STORE:
    subs N, N, #32
    vst1.32 {VSRC_4S_C0_0, VSRC_4S_C0_1}, [C]!
    vst1.32 {VSRC_4S_C0_2}, [C], N
    vst1.32 {VSRC_4S_C1_0, VSRC_4S_C1_1}, [C]!
    vst1.32 {VSRC_4S_C1_2}, [C]

__END:
    sub r4, fp, #156
    vldm r4, {s0-s19}
    sub sp, fp, #28
    pop {r4-r9, fp}
    bx lr